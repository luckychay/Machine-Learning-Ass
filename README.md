
# Machine-Learning-Ass

## Usage
To run the code, requirements should be meet as indicated in requirements.txt.

`pip install -r requirements.txt`

For training,

`python classifier.py --mode train --classifier <classifier name>`

where "classifier name" refers to different classifiers, including "LDA", "Bayes", "DT".

For testing,

`python classifier.py --mode test --classifier <classifier name>`

For visualization,

`python classifier.py --mode vis`

## Points about LDA
There are two different definitions about with-class and between-class scatter matrix

The first one is as follows, which is used by this repo:
![1648607797(1)](https://user-images.githubusercontent.com/55084546/160739741-c123a144-8bca-4532-847b-459526a73ebc.png)

The second one is as follows, which is used by this repo and sklearn. The difference lays not only on multiplying Sw and Sb with priors probabilities, but also on changing the mean vector for deduction in Sb. 
![1648608329(1)](https://user-images.githubusercontent.com/55084546/160740728-60df572e-7188-4b15-9a04-4a0f26542261.png)

Note that, generally we can use LDA both for dimensionality reduction and classificiation, when for classification we use **all** the eigenvectors and while choosing the **leading c-1**(c is the total class number) eigenvectors for dimensionality reduction.

## Points about matrix ğ€â€²ğ€

Geometrically, matrix ğ€â€²ğ€ is called matrix of scalar products (= dot products, = inner products). Algebraically, it is called sum-of-squares-and-cross-products matrix (SSCP).

Its ğ‘–-th diagonal element is equal to âˆ‘ğ‘2(ğ‘–), where ğ‘(ğ‘–) denotes values in the ğ‘–-th column of ğ€ and âˆ‘ is the sum across rows. The ğ‘–ğ‘—-th off-diagonal element therein is âˆ‘ğ‘(ğ‘–)ğ‘(ğ‘—).

There is a number of important association coefficients and their square matrices are called angular similarities or SSCP-type similarities:

Dividing SSCP matrix by ğ‘›, the sample size or number of rows of ğ€, you get MSCP (mean-square-and-cross-product) matrix. The pairwise formula of this association measure is hence âˆ‘ğ‘¥ğ‘¦/ğ‘› (with vectors ğ‘¥ and ğ‘¦ being a pair of columns from ğ€).

If you center columns (variables) of ğ€, then ğ€â€²ğ€ is the scatter (or co-scatter, if to be rigorous) matrix and ğ€â€²ğ€/(ğ‘›âˆ’1) is the covariance matrix. Pairwise formula of covariance is âˆ‘ğ‘ğ‘¥ğ‘ğ‘¦/ğ‘›âˆ’1 with ğ‘ğ‘¥ and ğ‘ğ‘¦ denoting centerted columns.

If you z-standardize columns of ğ€ (subtract the column mean and divide by the standard deviation), then ğ€â€²ğ€/(ğ‘›âˆ’1) is the Pearson correlation matrix: correlation is covariance for standardized variables. Pairwise formula of correlation is âˆ‘ğ‘§ğ‘¥ğ‘§ğ‘¦/ğ‘›âˆ’1 with ğ‘§ğ‘¥ and ğ‘§ğ‘¦ denoting standardized columns. The correlation is also called coefficient of linearity.

If you unit-scale columns of ğ€ (bring their SS, sum-of-squares, to 1), then ğ€â€²ğ€ is the cosine similarity matrix. The equivalent pairwise formula thus appears to be âˆ‘ğ‘¢ğ‘¥ğ‘¢ğ‘¦=âˆ‘ğ‘¥ğ‘¦/âˆšâˆ‘ğ‘¥2âˆšâˆ‘ğ‘¦2 with ğ‘¢ğ‘¥ and ğ‘¢ğ‘¦ denoting L2-normalized columns. Cosine similarity is also called coefficient of proportionality.

If you center and then unit-scale columns of ğ€, then ğ€â€²ğ€ is again the Pearson correlation matrix, because correlation is cosine for centered variables1,2: âˆ‘ğ‘ğ‘¢ğ‘¥ğ‘ğ‘¢ğ‘¦=âˆ‘ğ‘ğ‘¥ğ‘ğ‘¦/âˆšâˆ‘ğ‘2ğ‘¥âˆšâˆ‘ğ‘2ğ‘¦
